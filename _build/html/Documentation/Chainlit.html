<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Chainlit: an easy way to interact with LLMs &mdash; ChatG2IA  documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../_static/jquery.js?v=5d32c60e"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../_static/documentation_options.js?v=5929fcd5"></script>
        <script src="../_static/doctools.js?v=888ff710"></script>
        <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Setup" href="Setup.html" />
    <link rel="prev" title="How to load data based on an adapter fine-tuned on a sample of synthetic data generated by GPT-4" href="How%20to%20load%20data%20based%20on%20an%20adapter%20fine-tuned%20on%20a%20sample%20of%20synthetic%20data%20generated%20by%20GPT-4.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            ChatG2IA
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Table of Content:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="Introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="Model.html">Model</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="Synthetic_data.html">Synthetic Data</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="GPT4.html">Data Generation for LLM Fine-tuning with Synthetic Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="GPT4.html#six-sigma-domain-integration">4. Six Sigma Domain Integration</a></li>
<li class="toctree-l2"><a class="reference internal" href="GPT4.html#examples">5. Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="GPT4.html#quality-metrics-and-evaluation">6. Quality Metrics and Evaluation</a></li>
<li class="toctree-l2"><a class="reference internal" href="GPT4.html#next-steps-and-recommendations">7. Next Steps and Recommendations</a></li>
<li class="toctree-l2"><a class="reference internal" href="How%20to%20load%20data%20based%20on%20an%20adapter%20fine-tuned%20on%20a%20sample%20of%20synthetic%20data%20generated%20by%20GPT-4.html">How to load data based on an adapter fine-tuned on a sample of synthetic data generated by GPT-4</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Chainlit: an easy way to interact with LLMs</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#prerequisites">Prerequisites</a></li>
<li class="toctree-l3"><a class="reference internal" href="#downloading-the-model">Downloading the model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#the-interface">The interface</a></li>
<li class="toctree-l3"><a class="reference internal" href="#running-the-interface">Running the interface</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="Setup.html">Setup</a></li>
<li class="toctree-l2"><a class="reference internal" href="Customization.html">Customization</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">ChatG2IA</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="Synthetic_data.html">Synthetic Data</a></li>
      <li class="breadcrumb-item active">Chainlit: an easy way to interact with LLMs</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/Documentation/Chainlit.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="chainlit-an-easy-way-to-interact-with-llms">
<h1>Chainlit: an easy way to interact with LLMs<a class="headerlink" href="#chainlit-an-easy-way-to-interact-with-llms" title="Link to this heading"></a></h1>
<p class="linemarker linemarker-4">In this section, we’ll demonstrate the process of engaging with an open-source language model of your choice available on the Hugging Face model hub. To facilitate this interaction, we’ll leverage the Chainlit library—an open-source asynchronous Python framework designed to expedite the creation of applications akin to ChatGPT. This library enables seamless interaction with models through an automatically generated user interface. For more detailed insights into Chainlit and its functionalities, further information is available <a class="reference external" href="https://github.com/Chainlit/chainlit">here</a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p class="linemarker linemarker-7">There is a video tutorial available for this section <a class="reference external" href="put_the_link_here">here</a>.</p>
</div>
<section id="prerequisites">
<h2>Prerequisites<a class="headerlink" href="#prerequisites" title="Link to this heading"></a></h2>
<p class="linemarker linemarker-12">To get started, we’ll need to install the Chainlit library and other dependencies. To do so, we’ll create a new virtual environment using the following command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># using python</span>
python3<span class="w"> </span>-m<span class="w"> </span>venv<span class="w"> </span>chainlit_env

<span class="c1"># using anaconda</span>
conda<span class="w"> </span>create<span class="w"> </span>-n<span class="w"> </span>chainlit_env<span class="w"> </span><span class="nv">python</span><span class="o">=</span><span class="m">3</span>.11
</pre></div>
</div>
<p class="linemarker linemarker-22">Next, we’ll activate the virtual environment and install the necessary dependencies</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># activate the virtual environment</span>
<span class="c1"># using python</span>
<span class="nb">source</span><span class="w"> </span>chainlit_env/bin/activate

<span class="c1"># using anaconda</span>
conda<span class="w"> </span>activate<span class="w"> </span>chainlit_env

<span class="c1"># install the dependencies</span>
pip<span class="w"> </span>install<span class="w"> </span>chainlit
pip<span class="w"> </span>install<span class="w"> </span>ctransformers
pip<span class="w"> </span>install<span class="w"> </span>langchain
pip<span class="w"> </span>install<span class="w"> </span>torch
</pre></div>
</div>
</section>
<section id="downloading-the-model">
<h2>Downloading the model<a class="headerlink" href="#downloading-the-model" title="Link to this heading"></a></h2>
<p class="linemarker linemarker-42">The models that we need to download from Hugging Face Hub should be in the <strong>GGUF</strong> format. In this <a class="reference external" href="https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/tree/main">link</a> you can find the <strong>Mistral</strong> model in this format, download the model that has this name <strong>mistral-7b-instruct-v0.1.Q4_K_S.gguf</strong> because we will need it for the rest of this tutorial. If you want to use another llm just search for it in one of TheBloke’s <a class="reference external" href="https://huggingface.co/TheBloke">repositories</a>.</p>
<figure class="align-center" id="custom-label">
<a class="reference internal image-reference" href="../_images/mistral_gguf.png"><img alt="Alternative text for the image" src="../_images/mistral_gguf.png" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-text">Caption text goes here.</span><a class="headerlink" href="#custom-label" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="the-interface">
<h2>The interface<a class="headerlink" href="#the-interface" title="Link to this heading"></a></h2>
<p class="linemarker linemarker-56">Now let’s create a new file called <strong>app.py</strong> and import the necessary libraries:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">chainlit</span> <span class="k">as</span> <span class="nn">cl</span>
<span class="kn">from</span> <span class="nn">chainlit.input_widget</span> <span class="kn">import</span> <span class="n">Slider</span><span class="p">,</span> <span class="n">Switch</span>
<span class="kn">from</span> <span class="nn">langchain.chains</span> <span class="kn">import</span> <span class="n">LLMChain</span>
<span class="kn">from</span> <span class="nn">langchain.llms</span> <span class="kn">import</span> <span class="n">CTransformers</span>
<span class="kn">from</span> <span class="nn">langchain.prompts</span> <span class="kn">import</span> <span class="n">PromptTemplate</span>
</pre></div>
</div>
<p class="linemarker linemarker-67">Now we’ll create a variable to store the path to the model we downloaded earlier:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">local_llm</span> <span class="o">=</span> <span class="s2">&quot;./mistral-7b-instruct-v0.1.Q4_K_S.gguf&quot;</span> <span class="c1"># download the model from this link https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/tree/main</span>
</pre></div>
</div>
<p class="linemarker linemarker-73">Next we’ll create a configuration dictionary to store the parameters that we’ll use to initialize our model:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;max_new_tokens&#39;</span><span class="p">:</span> <span class="mi">128</span><span class="p">,</span>
    <span class="s1">&#39;repetition_penalty&#39;</span><span class="p">:</span> <span class="mf">1.1</span><span class="p">,</span>
    <span class="s1">&#39;temperature&#39;</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span>
    <span class="s1">&#39;top_p&#39;</span><span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span>
    <span class="s1">&#39;top_k&#39;</span><span class="p">:</span> <span class="mi">50</span><span class="p">,</span>
    <span class="s1">&#39;stream&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
    <span class="s1">&#39;threads&#39;</span><span class="p">:</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">cpu_count</span><span class="p">()</span> <span class="o">/</span> <span class="mi">2</span><span class="p">),</span>
<span class="p">}</span>
</pre></div>
</div>
<p class="linemarker linemarker-87">The values in this dictionary are the default values for the parameters that we’ll use to initialize our model. For more information on these parameters, please refer to the CTransformers <a class="reference external" href="https://github.com/marella/ctransformers#config">documentation</a>.</p>
<p class="linemarker linemarker-89">In the interface we’ll use the <strong>Slider</strong> and <strong>Switch</strong> widgets to allow the user to adjust these parameters. To do so, we’ll use Chainlit’s ChatSettings class as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">settings</span> <span class="o">=</span> <span class="k">await</span> <span class="n">cl</span><span class="o">.</span><span class="n">ChatSettings</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="n">Slider</span><span class="p">(</span>
            <span class="nb">id</span><span class="o">=</span><span class="s2">&quot;Temperature&quot;</span><span class="p">,</span>
            <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Temperature&quot;</span><span class="p">,</span>
            <span class="n">initial</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;temperature&#39;</span><span class="p">],</span>
            <span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="nb">max</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
            <span class="n">step</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
        <span class="p">),</span>
        <span class="n">Slider</span><span class="p">(</span>
            <span class="nb">id</span><span class="o">=</span><span class="s2">&quot;Repetition Penalty&quot;</span><span class="p">,</span>
            <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Repetition Penalty&quot;</span><span class="p">,</span>
            <span class="n">initial</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;repetition_penalty&#39;</span><span class="p">],</span>
            <span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="nb">max</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
            <span class="n">step</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
        <span class="p">),</span>
        <span class="n">Slider</span><span class="p">(</span>
            <span class="nb">id</span><span class="o">=</span><span class="s2">&quot;Top P&quot;</span><span class="p">,</span>
            <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Top P&quot;</span><span class="p">,</span>
            <span class="n">initial</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;top_p&#39;</span><span class="p">],</span>
            <span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="nb">max</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">step</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
        <span class="p">),</span>
        <span class="n">Slider</span><span class="p">(</span>
            <span class="nb">id</span><span class="o">=</span><span class="s2">&quot;Top K&quot;</span><span class="p">,</span>
            <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Top K&quot;</span><span class="p">,</span>
            <span class="n">initial</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;top_k&#39;</span><span class="p">],</span>
            <span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="nb">max</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
            <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="p">),</span>
        <span class="n">Slider</span><span class="p">(</span>
            <span class="nb">id</span><span class="o">=</span><span class="s2">&quot;Max New Tokens&quot;</span><span class="p">,</span>
            <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Max New Tokens&quot;</span><span class="p">,</span>
            <span class="n">initial</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;max_new_tokens&#39;</span><span class="p">],</span>
            <span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="nb">max</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span>
            <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="p">),</span>
        <span class="n">Switch</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="s2">&quot;Streaming&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Stream Tokens&quot;</span><span class="p">,</span> <span class="n">initial</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
    <span class="p">]</span>
<span class="p">)</span><span class="o">.</span><span class="n">send</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-center" id="the-settings-panel">
<a class="reference internal image-reference" href="../_images/configuration_sliders.png"><img alt="Alternative text for the image" src="../_images/configuration_sliders.png" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-text">The settings panel.</span><a class="headerlink" href="#the-settings-panel" title="Link to this image"></a></p>
</figcaption>
</figure>
<p class="linemarker linemarker-148">Now whenever the user changes one of these parameters, we need to setup the model to use the new values. To do so, we’ll create a function called <strong>setup_agent</strong> that will update the values in the <strong>config</strong> dictionary as well as applying it to the model:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">setup_agent</span><span class="p">(</span><span class="n">settings</span><span class="p">):</span>
    <span class="c1"># update the config dictionary with the new settings</span>
    <span class="n">config</span><span class="p">[</span><span class="s1">&#39;temperature&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">settings</span><span class="p">[</span><span class="s1">&#39;Temperature&#39;</span><span class="p">]</span>
    <span class="n">config</span><span class="p">[</span><span class="s1">&#39;repetition_penalty&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">settings</span><span class="p">[</span><span class="s1">&#39;Repetition Penalty&#39;</span><span class="p">]</span>
    <span class="n">config</span><span class="p">[</span><span class="s1">&#39;top_p&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">settings</span><span class="p">[</span><span class="s1">&#39;Top P&#39;</span><span class="p">]</span>
    <span class="n">config</span><span class="p">[</span><span class="s1">&#39;top_k&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">settings</span><span class="p">[</span><span class="s1">&#39;Top K&#39;</span><span class="p">]</span>
    <span class="n">config</span><span class="p">[</span><span class="s1">&#39;max_new_tokens&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">settings</span><span class="p">[</span><span class="s1">&#39;Max New Tokens&#39;</span><span class="p">]</span>
    <span class="n">config</span><span class="p">[</span><span class="s1">&#39;stream&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">settings</span><span class="p">[</span><span class="s1">&#39;Streaming&#39;</span><span class="p">]</span>

    <span class="c1"># update the model with the new settings</span>
    <span class="n">llm_init</span> <span class="o">=</span> <span class="n">CTransformers</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="n">local_llm</span><span class="p">,</span>
        <span class="n">model_type</span><span class="o">=</span><span class="s2">&quot;mistral&quot;</span><span class="p">,</span>
        <span class="n">lib</span><span class="o">=</span><span class="s2">&quot;avx2&quot;</span><span class="p">,</span>  <span class="c1"># &#39;avx2&#39; or &#39;avx512&#39;</span>
        <span class="o">**</span><span class="n">config</span>
    <span class="p">)</span>

    <span class="c1"># creating the prompt template</span>
    <span class="n">template</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    Question: </span><span class="si">{question}</span>
<span class="s2">    Answer:</span>
<span class="s2">    &quot;&quot;&quot;</span>

    <span class="n">prompt</span> <span class="o">=</span> <span class="n">PromptTemplate</span><span class="p">(</span><span class="n">template</span><span class="o">=</span><span class="n">template</span><span class="p">,</span> <span class="n">input_variables</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;question&#39;</span><span class="p">])</span>

    <span class="c1"># creating the llm chain</span>
    <span class="n">llm_chain</span> <span class="o">=</span> <span class="n">LLMChain</span><span class="p">(</span><span class="n">prompt</span><span class="o">=</span><span class="n">prompt</span><span class="p">,</span> <span class="n">llm</span><span class="o">=</span><span class="n">llm_init</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="c1"># saving the llm chain in the session</span>
    <span class="n">cl</span><span class="o">.</span><span class="n">user_session</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s1">&#39;llm_chain&#39;</span><span class="p">,</span> <span class="n">llm_chain</span><span class="p">)</span>
</pre></div>
</div>
<p class="linemarker linemarker-183">In the <strong>setup_agent</strong> function, we have created a <strong>PromptTemplate</strong> object that will be used to generate the prompt that we’ll feed to the model. This object takes a template string and a list of input variables. The template string is a string that contains the text that we want to feed to the model. The input variables are the variables that we want to replace in the template string. In our case, we want to replace the <strong>{question}</strong> variable with the question that the user will ask. For more information on the PromptTemplate class, please refer to the LangChain <a class="reference external" href="https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/#prompttemplate">documentation</a>.</p>
<p class="linemarker linemarker-185">After that, we have created an <strong>LLMChain</strong> object that will be used to interact with the model. This object takes a <strong>PromptTemplate</strong> object and an <strong>LLM</strong> object. For more information on the LLMChain class, please refer to the LangChain <a class="reference external" href="https://python.langchain.com/docs/modules/chains/foundational/llm_chain#legacy-llmchain">documentation</a>.</p>
<p class="linemarker linemarker-187">Finally, we have saved the <strong>LLMChain</strong> object in the user session so that we can access it later. For more information on the user session, please refer to the Chainlit <a class="reference external" href="https://docs.chainlit.io/backend/user-session">documentation</a>.</p>
<p class="linemarker linemarker-189">The <strong>setup_agent</strong> will be called whenever the user changes one of the parameters in the interface. To do so, we’ll use the <strong>cl.on_settings_update</strong> decorator as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@cl</span><span class="o">.</span><span class="n">on_chat_start</span>
<span class="k">async</span> <span class="k">def</span> <span class="nf">start</span><span class="p">():</span>
    <span class="n">settings</span> <span class="o">=</span> <span class="k">await</span> <span class="n">cl</span><span class="o">.</span><span class="n">ChatSettings</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>

    <span class="c1"># calling the setup_agent function</span>
    <span class="k">await</span> <span class="n">setup_agent</span><span class="p">(</span><span class="n">settings</span><span class="p">)</span>


<span class="nd">@cl</span><span class="o">.</span><span class="n">on_settings_update</span>
<span class="k">async</span> <span class="k">def</span> <span class="nf">setup_agent</span><span class="p">(</span><span class="n">settings</span><span class="p">):</span>
    <span class="c1"># the content of the setup_agent function</span>
</pre></div>
</div>
<p class="linemarker linemarker-205">Now, we are ready to start the chat. To do so, we’ll use the <strong>cl.on_message</strong> decorator as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@cl</span><span class="o">.</span><span class="n">on_message</span>
<span class="k">async</span> <span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">message</span><span class="p">):</span>
    <span class="c1"># getting the llm chain from the session</span>
    <span class="n">llm_chain</span> <span class="o">=</span> <span class="n">cl</span><span class="o">.</span><span class="n">user_session</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;llm_chain&#39;</span><span class="p">)</span>

    <span class="c1"># generating the response</span>
    <span class="n">result</span> <span class="o">=</span> <span class="k">await</span> <span class="n">llm_chain</span><span class="o">.</span><span class="n">acall</span><span class="p">(</span><span class="n">message</span><span class="o">.</span><span class="n">content</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">cl</span><span class="o">.</span><span class="n">AsyncLangchainCallbackHandler</span><span class="p">()])</span>

    <span class="c1"># sending the response</span>
    <span class="k">await</span> <span class="n">cl</span><span class="o">.</span><span class="n">Message</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="n">result</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">send</span><span class="p">()</span>
</pre></div>
</div>
<p class="linemarker linemarker-220">In the <strong>main</strong> function, we have retrieved the <strong>LLMChain</strong> object from the user session and used it to generate the response. The <strong>acall</strong> method takes the user input and a list of callbacks. The <strong>AsyncLangchainCallbackHandler</strong> is a callback that is used to handle the asynchronous calls to the model. For more information on the <strong>LangChain Callback Handler</strong>, please refer to the Chainlit <a class="reference external" href="https://docs.chainlit.io/api-reference/integrations/langchain">documentation</a>.</p>
<p class="linemarker linemarker-222">Finally, we have sent the response to the user using the <strong>cl.Message</strong> class. For more information on the <strong>Message</strong> class, please refer to the Chainlit <a class="reference external" href="https://docs.chainlit.io/api-reference/message">documentation</a>.</p>
</section>
<section id="running-the-interface">
<h2>Running the interface<a class="headerlink" href="#running-the-interface" title="Link to this heading"></a></h2>
<p class="linemarker linemarker-227">To run the interface, we’ll use the following command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>chainlit<span class="w"> </span>run<span class="w"> </span>app.py<span class="w"> </span>-w
</pre></div>
</div>
<p class="linemarker linemarker-233">After running the command, you should see something like this:</p>
<figure class="align-center" id="interface">
<a class="reference internal image-reference" href="../_images/chainlit_interface.png"><img alt="Alternative text for the image" src="../_images/chainlit_interface.png" style="width: 100%;" /></a>
<figcaption>
<p><span class="caption-text">The interface once loaded.</span><a class="headerlink" href="#interface" title="Link to this image"></a></p>
</figcaption>
</figure>
<p class="linemarker linemarker-244">To change the content that appears in the interface once running the command, you can edit the <strong>chainlit.md</strong> file</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="How%20to%20load%20data%20based%20on%20an%20adapter%20fine-tuned%20on%20a%20sample%20of%20synthetic%20data%20generated%20by%20GPT-4.html" class="btn btn-neutral float-left" title="How to load data based on an adapter fine-tuned on a sample of synthetic data generated by GPT-4" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="Setup.html" class="btn btn-neutral float-right" title="Setup" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, GIIA.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>